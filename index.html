<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>LM Pub Quiz</title>

  <meta property="og:title" content="LM Pub Quiz" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://lm-pub-quiz.github.io/" />
  <meta property="og:image" content="https://lm-pub-quiz.github.io/media/preview.png" />

  <link rel="stylesheet" href="https://fonts.xz.style/serve/inter.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@exampledev/new.css@1.1.2/new.min.css">
  <link rel="stylesheet" href="style.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark-dimmed.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

  <!-- and it's easy to individually load additional languages -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

  <script src="https://cdn.jsdelivr.net/npm/d3@7"></script>
  <link rel="preload" href="present_data.js" as="script" />
  <link rel="preload" href="results.json" as="script" />

  <script>hljs.highlightAll();</script>
</head>
<body>
    <header>
      <div class="title-div">
        <h1><span class="title-lm">LM</span><br><span class="title-pq">Pub Quiz</span></h1>
      </div>
      <div class="subtitle-div">
        <h2 class="subtitle">Evaluate language models using multiple choice items</h2>
        <nav>
            <a href="https://github.com/lm-pub-quiz/lm-pub-quiz"><i class="bi bi-git"></i>&nbsp;Library</a> /
            <a href="https://lm-pub-quiz.github.io/lm-pub-quiz"><i class="bi bi-journal-text"></i>&nbsp;Documentation</a> /
            <a href="https://github.com/lm-pub-quiz/BEAR"><i class="bi bi-database"></i>&nbsp;BEAR Dataset</a> /
            <a href="https://aclanthology.org/2024.findings-naacl.155/"><i class="bi bi-file-earmark"></i>&nbsp;Paper</a> <!-- /
            <a href=""><i class="bi bi-file-zip"></i>&nbsp;Raw Results</a> -->
        </nav>
      </div>
    </header>

    <section>
      <figure class="shadow-box">
        <img src="./media/bear_evaluation_final.svg" width="100%" alt="Illustration of how LM Pub Quiz evaluates LMs.">
        <figcaption>Illustration of how LM Pub Quiz evaluates LMs: Answers are ranked by the (pseudo) log-likelihoods of the textual statements derived from all of the answer options.</figcaption> 
      </figure>
    </section>
    <section class="shadow-box">
      <h2>Leaderboard</h2>
      <p>
        We evaluated a variety of language models, trained using different pretraining objectives and representing both causal and masked LM types, on the BEAR dataset.
      </p>
      <div style="overflow-x: scroll;">

<table border="1" class="leaderboard">
  <colgroup>
     <col span="1" style="min-width: 4rem;">
     <col span="1" style="min-width: 3rem;">
     <col span="1" style="min-width: 3rem;">
     <col span="1" style="min-width: 3rem;">
     <col span="1" style="min-width: 3rem;">
     <col span="1" style="min-width: 4rem;">
  </colgroup>
  <thead>
    <tr style="text-align: left;">
      <th data-sort-key="model_name">Model</th>
      <th data-sort-key="model_type">Type</th>
      <th data-sort-key="year_published">Year of Publication</th>
      <th data-sort-key="num_params">Num Params</th>
      <th data-sort-key="num_tokens">Num Tokens</th>
      <th data-sort-key="accuracy">BEAR</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table>
      </div>
    </section>
    <section class="shadow-box">
      <h2>Contributing to the Leaderboard</h2>
      <p>
        <b>We warmly welcome contributions to the BEAR leaderboard!</b> To contribute new models to the leaderboard, please add your results to the file <i>results.json</i>
        and <a href="https://github.com/lm-pub-quiz/lm-pub-quiz.github.io/pulls"> create a pull request</a>.
      </p>
      <p>The result entry should follow the following format:</p>
      <pre><code class="language-json">    {
        "model_name": "bert-base-cased",
        "model_url": "https:\/\/huggingface.co\/bert-base-cased",
        "model_family":"bert",
        "model_type":"MLM",
        "year_published":"2018",
        "num_params": 109e6,
        "num_tokens": 3.3e9,
        "size_pretraining_GB": null,
        "source":"https://huggingface.co/blog/bert-101",
        "accuracy":{
            "mean":0.1839348079,
            "sem":0.0036593149
        }
    },</code></pre>
      <p style="font-size: 0.85em">
        Please add the model name & family, the type of model (CLM or MLM) and the URL where the model can be accessed. Please also add information on the number of parameters the model contains, as well as information on the pre-training setup: The number of pre-training tokens as well as the size of the pre-training data in GB (if this information is available).
        If this information was retrieved from some other source (than the main page of the model), please add the URL to the source field.
        The <i>mean accuracy</i> refers to the weighted mean over all three templates, <i>sem</i> refers to the standard error.
      </p>
    </section>
    <section class="shadow-box">
      <div style="text-align: right;"><span class="badge acl">Accepted at NAACL 2024</span></div>
      <h2>BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models</h2>
      <h3>Abstract</h3>
      <p>
  Knowledge probing assesses to which degree a language model (LM) has successfully learned relational knowledge during pre-training. Probing is an inexpensive way to compare LMs of different sizes and training configurations. However, previous approaches rely on the objective function used in pre-training LMs and are thus applicable only to masked or causal LMs. As a result, comparing different types of LMs becomes impossible. To address this, we propose an approach that uses an LM's inherent ability to estimate the log-likelihood of any given textual statement. We carefully design an evaluation dataset of 7,731 instances (40,916 in a larger variant) from which we produce alternative statements for each relational fact, one of which is correct. We then evaluate whether an LM correctly assigns the highest log-likelihood to the correct statement. Our experimental evaluation of 22 common LMs shows that our proposed framework, BEAR, can effectively probe for knowledge across different LM types. We release the BEAR datasets and an open-source framework that implements the probing approach to the research community to facilitate the evaluation and development of LMs. 
      </p>
      <p>
        <a href="">
          <button><i class="bi bi-file-earmark"></i> Read the Paper</button>
        </a>
      </p>
    </section>

    <section class="shadow-box">
      <figure>
        <div style="width:100%">
          <svg class="result-plot" preserveAspectRatio="xMinYMin meet"></svg>
        </div>
        <figcaption>
          Accuracy of various models on the BEAR dataset over the
          <select id="xAxisKeySelect">
            <option value="num_params">number of parameters</option>
            <option value="num_tokens">number of tokens during pretraining</option>
            <option value="year_published">year of publication</option>
          </select>
          .</figcaption> 
      </figure>
    </section>

    <section class="shadow-box">
      <h2>Example Usage</h2>
      <p>Install the package via pip:</p>
      <pre><code class="language-bash">pip install lm-pub-quiz</code></pre>
      <p>Evaluate a model on BEAR:</p>
      <pre><code class="language-python">from lm_pub_quiz import Dataset, Evaluator

# Load the dataset
bear = Dataset.from_name("BEAR")

# Load the model
evaluator = Evaluator.from_model("gpt2", model_type="CLM", device="cuda:0")

# Run the evaluation
result = evaluator.evaluate_dataset(bear, template_index=0, batch_size=32, save_path="results/gpt2")

# Show the overall accuracy
print(result.get_metrics("accuracy", accumulate_all=True))</code></pre>

      <p>This example script outputs the accuracy accumulated over all relations weighed by the number of instances (this is what we call the "BEAR-score") as a <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html">pandas.Series</a>:</p>
      <pre><code class="language-plaintext">accuracy            0.149528
num_instances    7731.000000
dtype: float64</code></pre>
      <p>For more details, visit the <a href="https://lm-pub-quiz.github.io/lm-pub-quiz/">documentation</a>.</p>
    </section>

    <section class="shadow-box">
      <h2>Citation</h2>
      <p>When using the dataset or library, please cite the following paper:</p>
      <pre><code class="language-plaintext">@inproceedings{wiland-etal-2024-bear,
    title = "{BEAR}: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models",
    author = "Wiland, Jacek  and
      Ploner, Max  and
      Akbik, Alan",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.155/",
    doi = "10.18653/v1/2024.findings-naacl.155",
    pages = "2393--2411"
}</code></pre>
    </section>

    <section class="contributors-section">
      <h2>Meet the Contributors</h2>
      <div class="contributors-container">
        <div class="shadow-box contributor-card">
          <img src="media/portrait_jacek.jpg" alt="Portrait of Jacek Wiland" class="portrait">
          <h3>Jacek Wiland</h3>
          <p class="contributor-role">Core Contributor</p>
          <ul class="contact-info">
            <li><a href="https://github.com/stw2/"><i class="bi bi-github"></i> Github Profile</a></li>
          </ul>
        </div>
        <div class="shadow-box contributor-card">
          <img src="media/portrait_max.jpg" alt="Portrait of Max Ploner" class="portrait">
          <h3>Max Ploner</h3>
          <p class="contributor-role">Core Contributor</p>
          <ul class="contact-info">
            <li><a href="https://maxploner.de/"><i class="bi bi-cursor"></i> Website</a>
            <li><a href="https://github.com/plonerma/"><i class="bi bi-github"></i> Github Profile</a></li>
          </ul>
        </div>
        <div class="shadow-box contributor-card">
          <img src="media/portrait_alan.jpg" alt="Portrait of Alan Akbik" class="portrait">
          <h3>Alan Akbik</h3>
          <p class="contributor-role">Core Contributor</p>
          <ul class="contact-info">
            <li><a href="https://alanakbik.github.io/"><i class="bi bi-cursor"></i> Website</a>
            <li><a href="https://github.com/alanakbik/"><i class="bi bi-github"></i> Github Profile</a></li>
          </ul>
        </div>
        <div class="shadow-box contributor-card">
          <h3>Sebastian Pohl</h3>
          <p class="contributor-role">Contributor</p>
          <ul class="contact-info">
            <li><a href="https://github.com/oneSebastian/"><i class="bi bi-github"></i> Github Profile</a></li>
          </ul>
        </div>
      </div>
    </section>
  <script src="present_data.js" type="text/javascript"></script>
</body>
</html>
